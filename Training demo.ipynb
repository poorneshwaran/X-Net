{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting X-Ray Images using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook to walk through the process of performing inference on a pretrianed model to segment X-Ray images into bone, soft-tissue and open beam regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Since we cannot relase the fully trained model for proprietary reasons, this pretrained model has only been trained on ~10 images. Perfomance is therefore significantly hindered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googledrivedownloader\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "gdd.download_file_from_google_drive(file_id='1Wel_XsyE7HcEq0TkZWI61GABO4jOtj9C',\n",
    "                                    dest_path='./dataset.hdf5')\n",
    "gdd.download_file_from_google_drive(file_id='1cePD5E-T9mr5W0xPGuzEnUt8Glpvn23U',\n",
    "                                    dest_path='./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import Keras sub-modules\n",
    "from keras.models import Model #functional API for Keras (best for greater flexibility)\n",
    "from keras.layers import Input, Concatenate, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense #'main' layers\n",
    "from keras.layers import BatchNormalization, Dropout #regulartisation layers\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import * #import all optimisers\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger #callbacks for model performance analysis\n",
    "from keras.metrics import categorical_accuracy #metrics for model performance\n",
    "from keras import backend as K #gives backend functionality\n",
    "from keras import losses #imports pre-defined loss functions\n",
    "from keras.models import load_model #allows pre-trained models to be called back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g5 files compress the data for convenient storage, as well as offering accessibility through a 'key' system. This is a useful file type, however, if it can become cumbersome for large datasets. Other storage mechanisms will still work with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_path = \"./dataset.hdf5\" ## this is our h5 file containing training and testing data\n",
    "dataset = h5py.File(hdf5_path , 'r')\n",
    "\n",
    "classes = 3\n",
    "\n",
    "test_images = dataset['test_img'][:]\n",
    "no_images, height, width, channels = test_images.shape\n",
    "\n",
    "test_labels =dataset['test_label'][:].reshape(-1,height*width, classes )\n",
    "\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above should have been relatively straight forward. We simply use the existing data files we have created, called them, and compiled the model based on this input. Before progressing further, we shall now investigate the model. By printing a model summary, Keras provides a user friendly output which shows the layers and their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this summary in mind, go to the ```XNet.py``` script and have a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of training and saving a model is summarised in the ```Training/TrainingClass.py``` script. Have a look at that script to see what the different functions are doing. In the class you will see that all the augementations are being performed on the data, and so you just need to create an HDF5 file which contains non-augmented images and call it into the training class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with your understanding of the training class we shall now try predicting from the model. In Keras this is very simple. Pass in the test image into the ```model.predict``` function and reshape the output to view it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an image to test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image = test_images[test_index]\n",
    "\n",
    "#as we are only running one image, we must reshape to shape (batch, height, width, channels)\n",
    "testing_image = testing_image.reshape((1,200,200,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(testing_image)\n",
    "\n",
    "#the prediction is a flattened array and so must be reshaped.\n",
    "#there are 3 channels as we are actually outputting the probability map over all 3 classes.\n",
    "prediction = prediction.reshape((200,200,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing prediction various postprocessing stages can be employed to fine tune the output. See the ```PostProcessing.py``` script for more details, which is then called in the ```Training/train.py``` script."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
